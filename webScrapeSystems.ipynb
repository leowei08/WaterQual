{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries for web-scraping\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import progressbar\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sqlalchemy import create_engine \n",
    "import threading\n",
    "\n",
    "# Connect to a local database to directly store systems info after scraping each one\n",
    "engine = create_engine('postgresql+psycopg2://postgres:password@localhost/EWG_new', client_encoding='utf 8') \n",
    "engine1 = create_engine('postgresql+psycopg2://postgres:password@localhost/Project', client_encoding='utf 8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameter for web-scraping\n",
    "BASE_URL = 'https://www.ewg.org/tapwater/search-results.php?zip5='\n",
    "SEARCH_URL_END = '&searchtype=zip'\n",
    "headers = {'User-Agent': 'Chrome/73.0.3683.103'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape for Zipcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 51/51 [02:34<00:00,  3.02s/it]\n"
     ]
    }
   ],
   "source": [
    "# Web-scraped the unitedstateszipcodes.org website for all the zipcodes\n",
    "zipcodes_2 = {'state':[], 'zipcode': []}\n",
    "for state in tqdm(zipcodes_1['state_id'].str.lower().unique()):\n",
    "    r = requests.get('https://www.unitedstateszipcodes.org/' + state + '/',headers=headers)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    for z in soup.find_all('div',{'class':'list-group-item'}):\n",
    "        zipcodes_2['state'].append(state)\n",
    "        zipcodes_2['zipcode'].append(z.find('a').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the dictionary into a dataframe\n",
    "zipcodes_2=pd.DataFrame(zipcodes_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split the zipcodes into different subsets for parallel computing\n",
    "a=zipcodes_2.iloc[:4170,:]\n",
    "b=zipcodes_2.iloc[4170:8340,:]\n",
    "c=zipcodes_2.iloc[8340:12510,:]\n",
    "d=zipcodes_2.iloc[12510:16680,:]\n",
    "e=zipcodes_2.iloc[16680:20850,:]\n",
    "f=zipcodes_2.iloc[20850:25020,:]\n",
    "g=zipcodes_2.iloc[25020:29190,:]\n",
    "h=zipcodes_2.iloc[29190:33360,:]\n",
    "i=zipcodes_2.iloc[33360:37530,:]\n",
    "j=zipcodes_2.iloc[37530:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to scrape systems info\n",
    "def scrape_systems(df):\n",
    "    for k in tqdm(df['zipcode'].unique()):\n",
    "        try:\n",
    "            url = BASE_URL + k + SEARCH_URL_END\n",
    "            r = requests.get(url,headers=headers)\n",
    "            soup = BeautifulSoup(r.content, 'html.parser')\n",
    "            if soup:\n",
    "                if soup.find_all('tbody'):\n",
    "                    systems=soup.find_all('tbody')[0].find_all('tr')\n",
    "                    for sys in systems:\n",
    "                        sys_info=sys.find_all('td')\n",
    "                        engine.execute(\"INSERT INTO systems (sys_url, sys_name, sys_pop, sys_zip, sys_state) VALUES (%s, %s, %s, %s, %s) on conflict do nothing\", sys_info[0].find('a')['href'], sys_info[0].text, sys_info[2].text, k, np.nan)\n",
    "                else:\n",
    "                    soup=soup.find('div',{'class':\"featured-utility\"})\n",
    "                    if soup:\n",
    "                        sys_info=soup.find_all('p')\n",
    "                        engine.execute(\"INSERT INTO systems (sys_url, sys_name, sys_pop, sys_zip, sys_state) VALUES (%s, %s, %s, %s, %s) on conflict do nothing\", soup.find_all('a')[0]['href'], sys_info[0].text, sys_info[2].text, k, np.nan)\n",
    "        except:\n",
    "            print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute parallel computing to scrape for multiple systems at once\n",
    "t1 = threading.Thread(target=scrape_systems, args=(a,)) \n",
    "t2 = threading.Thread(target=scrape_systems, args=(b,)) \n",
    "t3 = threading.Thread(target=scrape_systems, args=(c,)) \n",
    "t4 = threading.Thread(target=scrape_systems, args=(d,)) \n",
    "t5 = threading.Thread(target=scrape_systems, args=(e,)) \n",
    "t6 = threading.Thread(target=scrape_systems, args=(f,))\n",
    "t7 = threading.Thread(target=scrape_systems, args=(g,)) \n",
    "t8 = threading.Thread(target=scrape_systems, args=(h,)) \n",
    "t9 = threading.Thread(target=scrape_systems, args=(i,)) \n",
    "t10 = threading.Thread(target=scrape_systems, args=(j,))\n",
    "\n",
    "t1.start()\n",
    "t2.start()\n",
    "t3.start()\n",
    "t4.start()\n",
    "t5.start()\n",
    "t6.start()\n",
    "t7.start()\n",
    "t8.start()\n",
    "t9.start()\n",
    "t10.start()\n",
    "\n",
    "t1.join()\n",
    "print(\"A is done\")\n",
    "t2.join()\n",
    "print(\"B is done\")\n",
    "t3.join()\n",
    "print(\"C is done\")\n",
    "t4.join()\n",
    "print(\"D is done\")\n",
    "t5.join()\n",
    "print(\"E is done\")\n",
    "t6.join()\n",
    "print(\"F is done\")\n",
    "t7.join()\n",
    "print(\"G is done\")\n",
    "t8.join()\n",
    "print(\"H is done\")\n",
    "t9.join()\n",
    "print(\"I is done\")\n",
    "t10.join()\n",
    "print(\"J is done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the sets of systems retrieved last time (for class work) and this time\n",
    "new=pd.read_sql_query('select * from systems', con=engine)\n",
    "new['sys_id']=new['sys_url'].str.extract(r'system.php\\?pws=(\\w{9})')\n",
    "old=pd.read_sql_query('select * from utility', con=engine1)\n",
    "pd.DataFrame(pd.concat([new['sys_id'],old['utility_id']]).unique()).to_csv('../Data/ListofSystems.csv',index=False)\n",
    "zipcodes_2.to_csv('../Data/zipcodes.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
