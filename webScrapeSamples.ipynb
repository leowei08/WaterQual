{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries necessary\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import progressbar\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sqlalchemy import create_engine \n",
    "import threading\n",
    "headers = {'User-Agent': 'Chrome/73.0.3683.103'}\n",
    "\n",
    "# Connect to a local database to directly store the web-scraped data\n",
    "engine = create_engine('postgresql+psycopg2://postgres:password@localhost/EWG_new', client_encoding='utf 8') \n",
    "engine1 = create_engine('postgresql+psycopg2://postgres:password@localhost/Project', client_encoding='utf 8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in a list of systems we want to scrape for\n",
    "df=pd.read_csv('../Data/ListofSystems.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "systems=df['0'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to scrape for sampling records\n",
    "def scrape_contam(systemids):\n",
    "    for sys in tqdm(systemids):\n",
    "        mess=requests.get('https://www.ewg.org/tapwater/system.php?pws=' + sys, headers=headers)\n",
    "        mess=BeautifulSoup(mess.content, 'html.parser')\n",
    "        block1=mess.find_all('div',{'class':'contaminant-grid-item'})\n",
    "        if block1:\n",
    "            for contam in block1:\n",
    "                name=contam.find('div',{'class':\"contaminant-name\"}).find('h3').text\n",
    "                url=contam.find('div',{'class':\"testing-summary-btn-wrapper\"}).find('a')['href']\n",
    "                contammess=requests.get('https://www.ewg.org/tapwater/' + url, headers=headers)\n",
    "                contammess=BeautifulSoup(contammess.content, 'html.parser')\n",
    "                entries=contammess.find_all('tbody')\n",
    "                if entries:\n",
    "                    try:\n",
    "                        entries=entries[1].find_all('tr')[0]\n",
    "                        date = entries.find_all('td', {\"data-label\":\"Date\"})\n",
    "                        lab = entries.find_all('td', {\"data-label\":\"Lab ID\"})\n",
    "                        value = entries.find_all('td', {\"data-label\":\"Result\"})\n",
    "                        if len(date) == len(lab) and len(lab) == len(value):\n",
    "                            for index in range (len(date)):\n",
    "                                engine.execute(\"INSERT INTO contaminants (utilityid, contaminant_name, date, labid, value) VALUES (%s, %s, %s, %s, %s)\", (sys, name, date[index].text, lab[index].text, value[index].text))\n",
    "                        elif len(date) == len(value) and len(lab) == 0:\n",
    "                            for index in range (len(date)):\n",
    "                                engine.execute(\"INSERT INTO contaminants (utilityid, contaminant_name, date, labid, value) VALUES (%s, %s, %s, %s, %s)\", (sys, name, date[index].text, None, value[index].text))\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "\n",
    "        block2=mess.find_all('div', {'class': 'slide-toggle'})\n",
    "        if block2:\n",
    "            block2 = block2[0].find_all('a')\n",
    "            for contam in block2:\n",
    "                contammess=requests.get('https://www.ewg.org/tapwater/' + contam['href'], headers=headers)\n",
    "                contammess=BeautifulSoup(contammess.content, 'html.parser')\n",
    "                entries=contammess.find_all('tbody')\n",
    "                if entries:\n",
    "                    try:\n",
    "                        entries=entries[1].find_all('tr')[0]\n",
    "                        date = entries.find_all('td', {\"data-label\":\"Date\"})\n",
    "                        lab = entries.find_all('td', {\"data-label\":\"Lab ID\"})\n",
    "                        value = entries.find_all('td', {\"data-label\":\"Result\"})\n",
    "                        if len(date) == len(lab) and len(lab) == len(value):\n",
    "                            for index in range (len(date)):\n",
    "                                engine.execute(\"INSERT INTO contaminants (utilityid, contaminant_name, date, labid, value) VALUES (%s, %s, %s, %s, %s)\", (sys, contam.text, date[index].text, lab[index].text, value[index].text))\n",
    "                        elif len(date) == len(value) and len(lab) == 0:\n",
    "                            for index in range (len(date)):\n",
    "                                engine.execute(\"INSERT INTO contaminants (utilityid, contaminant_name, date, labid, value) VALUES (%s, %s, %s, %s, %s)\", (sys, contam.text, date[index].text, None, value[index].text))\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the file into many subsets for parallel computing\n",
    "a=systems[:4800]\n",
    "b=systems[4800:9600]\n",
    "c=systems[9600:14400]\n",
    "d=systems[14400:19200]\n",
    "e=systems[19200:24000]\n",
    "f=systems[24000:28800]\n",
    "g=systems[28800:33600]\n",
    "h=systems[33600:38400]\n",
    "i=systems[38400:43200]\n",
    "j=systems[43200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute parallel computing to scrape for multiple sampling records at once\n",
    "t1 = threading.Thread(target=scrape_contam, args=(a,)) \n",
    "t2 = threading.Thread(target=scrape_contam, args=(b,)) \n",
    "t3 = threading.Thread(target=scrape_contam, args=(c,)) \n",
    "t4 = threading.Thread(target=scrape_contam, args=(d,)) \n",
    "t5 = threading.Thread(target=scrape_contam, args=(e,)) \n",
    "t6 = threading.Thread(target=scrape_contam, args=(f,))\n",
    "t7 = threading.Thread(target=scrape_contam, args=(g,)) \n",
    "t8 = threading.Thread(target=scrape_contam, args=(h,)) \n",
    "t9 = threading.Thread(target=scrape_contam, args=(i,)) \n",
    "t10 = threading.Thread(target=scrape_contam, args=(j,))\n",
    "\n",
    "t1.start()\n",
    "t2.start()\n",
    "t3.start()\n",
    "t4.start()\n",
    "t5.start()\n",
    "t6.start()\n",
    "t7.start()\n",
    "t8.start()\n",
    "t9.start()\n",
    "t10.start()\n",
    "\n",
    "t1.join()\n",
    "print(\"A is done\")\n",
    "t2.join()\n",
    "print(\"B is done\")\n",
    "t3.join()\n",
    "print(\"C is done\")\n",
    "t4.join()\n",
    "print(\"D is done\")\n",
    "t5.join()\n",
    "print(\"E is done\")\n",
    "t6.join()\n",
    "print(\"F is done\")\n",
    "t7.join()\n",
    "print(\"G is done\")\n",
    "t8.join()\n",
    "print(\"H is done\")\n",
    "t9.join()\n",
    "print(\"I is done\")\n",
    "t10.join()\n",
    "print(\"J is done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for excluded systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the database for systems that were successfully scraped\n",
    "round1=pd.read_sql_query(\"select utilityid from contaminants\", con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the set difference between the complete set of systems and the current set we obtained\n",
    "exclude=np.setdiff1d(systems,round1['utilityid'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1646"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The number of systems not scraped\n",
    "len(exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the excluded systems for record\n",
    "pd.DataFrame(exclude,columns=['utilityid']).to_csv('Systems_excluded.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape for these systems again and through parallel computing\n",
    "a=exclude[:170]\n",
    "b=exclude[170:340]\n",
    "c=exclude[340:510]\n",
    "d=exclude[510:680]\n",
    "e=exclude[680:850]\n",
    "f=exclude[850:1020]\n",
    "g=exclude[1020:1190]\n",
    "h=exclude[1190:1360]\n",
    "i=exclude[1360:1530]\n",
    "j=exclude[1530:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute parallel computing\n",
    "t1 = threading.Thread(target=scrape_contam, args=(a,)) \n",
    "t2 = threading.Thread(target=scrape_contam, args=(b,)) \n",
    "t3 = threading.Thread(target=scrape_contam, args=(c,)) \n",
    "t4 = threading.Thread(target=scrape_contam, args=(d,)) \n",
    "t5 = threading.Thread(target=scrape_contam, args=(e,)) \n",
    "t6 = threading.Thread(target=scrape_contam, args=(f,))\n",
    "t7 = threading.Thread(target=scrape_contam, args=(g,)) \n",
    "t8 = threading.Thread(target=scrape_contam, args=(h,)) \n",
    "t9 = threading.Thread(target=scrape_contam, args=(i,)) \n",
    "t10 = threading.Thread(target=scrape_contam, args=(j,))\n",
    "\n",
    "t1.start()\n",
    "t2.start()\n",
    "t3.start()\n",
    "t4.start()\n",
    "t5.start()\n",
    "t6.start()\n",
    "t7.start()\n",
    "t8.start()\n",
    "t9.start()\n",
    "t10.start()\n",
    "\n",
    "t1.join()\n",
    "print(\"A is done\")\n",
    "t2.join()\n",
    "print(\"B is done\")\n",
    "t3.join()\n",
    "print(\"C is done\")\n",
    "t4.join()\n",
    "print(\"D is done\")\n",
    "t5.join()\n",
    "print(\"E is done\")\n",
    "t6.join()\n",
    "print(\"F is done\")\n",
    "t7.join()\n",
    "print(\"G is done\")\n",
    "t8.join()\n",
    "print(\"H is done\")\n",
    "t9.join()\n",
    "print(\"I is done\")\n",
    "t10.join()\n",
    "print(\"J is done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
